# 性能与监控

### Bigtable的性能瓶颈在哪里？

Bigtable的性能瓶颈通常是和CPU相关，而和I/O是没关系的。你在监控的页面是没有发现和I/O相关的监控指标的，这是因为Bigtable是能够将数据尽量“打散”到分布式存储上面，而分布式存储可以提供没有限制的I/O能力的。

关于CPU等相关监控指标，可以参考这个[文档](https://cloud.google.com/bigtable/docs/monitoring-instance?hl=zh-cn)。

所以，我们在性能监控时候会努力找到节点的平均CPU使用率和最热节点的CPU使用率，尤其是后者。举个例子，如果在监控中发现3个节点的部署中最热节点的CPU使用率接近100%，这肯定是出现了性能瓶颈。造成这种瓶颈的原因是，大多数请求都去到了最热节点上。这很可能是和数据没有均匀分布有关系。

那如何进一步排查呢？

* 找到最热的块，看看大概是什么表和什么主键范围，试图了解是否和业务相关。

```
  gcloud bigtable hot-tablets list CLUSTER_ID --instance INSTANCE_ID
```

* 判断主键是否合适，是足够分散的
* 查看相关表的大小，表过于小，例如只有几个GB，是不能产生足够的分片，并均匀分布到多个节点的
* 如果表足够大，这时候就需要考虑是否是由于读写负载不够，也同样没有产生足够的分片和均衡，需要对这个表进行“预热”。
* 另外，为了产生足够多的分片，bigtable建表的时候是可以预先生成分片，请参考下面的做法:

```
cbt createtable [TABLE_NAME] splits=[SPLITS]
```

### Bigtable提供的读写延时是什么水平？

Single row的读和写的操作延时小于10ms。YCSB的工作负载测试的结果通常是P99<10ms；在客户的场景下，这个取决于具体的负载和代码质量，但**至少**是P50<10ms。

### 对性能测试有什么通常的建议吗？

* 每个节点至少能有100GB的数据，例如我有1个300GB的Table，使用3个Node的Bigtable instance测试
* 从单个表开始测试
* 做测试取得性能指标前，建议对数据库预热，让Bigtable可以更好产生分片和均衡分片；通俗说，就是可以先压测一次，然后再压测一次，第二次的结果更接近真实的性能指标
* 每次压测时间要持续20分钟以上
